from llama_cpp import llama_cpp

print(llama_cpp.llama_supports_gpu_offload())
